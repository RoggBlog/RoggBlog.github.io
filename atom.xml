<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Logg 罗格</title>
  
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://stark365.com/"/>
  <updated>2019-04-02T07:00:37.172Z</updated>
  <id>http://stark365.com/</id>
  
  <author>
    <name>Logg 罗格</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>线性回归</title>
    <link href="http://stark365.com/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://stark365.com/线性回归/</id>
    <published>2019-04-02T05:15:23.285Z</published>
    <updated>2019-04-02T07:00:37.172Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>寻找一种能预测的趋势</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>线性回归通过一个或者多个<strong>自变量</strong>(<em>特征值</em>)与<strong>因变量</strong>(<em>目标值</em>)之间进行建模的回归分析，其中可以认为一个或者多个自变量之间的线性组合。</p><p>一元线性回归：涉及到的变量只有一个</p><p>多元线性回归：涉及到多个变量</p><h2 id="线性关系模型"><a href="#线性关系模型" class="headerlink" title="线性关系模型"></a>线性关系模型</h2><p>一个通过属性的线性组合来进行预测的函数</p><p>公式；</p><p>$$ f(x)=w_1x_1+w_2x_2+···+w_\alpha x_\alpha + b $$</p><blockquote><p>\(w\) 为权重，\(b\)为偏置顶</p></blockquote><h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><p>(m行, l列)*(l行, n列) = (m行, n列)</p><h2 id="通用公式"><a href="#通用公式" class="headerlink" title="通用公式"></a>通用公式</h2><p>$$ h(w)=w_0+w_1x_1+w_2x_2+···=w^Tx $$</p><p>其中\(w,x\)为矩阵：</p><p>$$ w=\begin{pmatrix}w_0\w_1\w_2\end{pmatrix} $$</p><p>$$ x=\begin{pmatrix}1\x_1\x_2\end{pmatrix} $$</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul><li>\(y_i为第i个训练样本的真实值\)</li><li>\(h_w(x_i)为i个训练样本特征值组合预测函数\)</li></ul><h3 id="总损失定义："><a href="#总损失定义：" class="headerlink" title="总损失定义："></a>总损失定义：</h3><p>$$ J(\theta)=(h_w(x_1)-y_1)^2+(h_w(x_2)-y_2)^2+···+(h_w(x_m)-y_m)^2 $$</p><p>$$=\sum_{i=1}^{m}(h_w(x_i)-y_i)^2$$</p><p>又称<strong>最小二乘法</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>随机森林</title>
    <link href="http://stark365.com/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://stark365.com/随机森林/</id>
    <published>2019-04-01T08:39:02.231Z</published>
    <updated>2019-04-01T09:33:56.695Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>随机森林是一个包含多个决策树的分类器，并且输出的类别是由个别树输出的类别的众数而定。最终结果是由全部决策树投票产生。</p><h2 id="集成学习方法"><a href="#集成学习方法" class="headerlink" title="集成学习方法"></a>集成学习方法</h2><p>集成学习方法通过建立几个模型组合来解决单一预测问题，他的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测，这些预测最后结合成单预测，因此优于任何一个单分类的预测。</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>假设有 N 个样本，M 个特征。</p><p>单棵树建立的过程：</p><ol><li>随机在 N 个样本中抽取一个，重复 N 次，有放回</li><li>随机在 M 个特征中选择 m 个(m &lt; M)特征</li></ol><p>建立10棵决策树</p><h2 id="sklearn-API"><a href="#sklearn-API" class="headerlink" title="sklearn API"></a>sklearn API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.ensemble.RandomForestClassifier</span><br></pre></td></tr></table></figure><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RandomForestClassifier(n_estimators=<span class="number">10</span>, criterion=<span class="string">'gini'</span>, max_depth=<span class="literal">None</span>, bootstrap=<span class="literal">True</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><ul><li>随机森林分类器</li><li>n_estimators: 正整数，森林的数目量，一般为120，200，300，500，800，1200</li><li>criteria：字符串，分割特征的测量方法</li><li>max_depth：正整数，树的最大深度，一般为5，8，15，25，30</li><li>max_features：每个决策树的最大特征数量<ul><li>‘aoto’ 默认选择特征数开根号</li><li>‘sqrt’ 开根号</li><li>‘log2’ 求以2为底的指数</li><li>None 取全部特征</li></ul></li><li>bootstrap：布尔类型，是否在构建树时使用放回抽样</li></ul><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li>具有极好的准确率</li><li>能够有效的运行在大数据集上</li><li>能够处理具有高纬度特征的输入样本，并且不需要降维</li><li>能够评估各个特征在分类问题上的重要性</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;随机森林&quot;&gt;&lt;a href=&quot;#随机森林
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://stark365.com/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://stark365.com/决策树/</id>
    <published>2019-04-01T02:16:23.838Z</published>
    <updated>2019-04-01T08:38:26.872Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>决策树思想的来源非常朴素，程序设计中的条件分支结构就是 if-then 结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。</p><h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><h3 id="信息量计算"><a href="#信息量计算" class="headerlink" title="信息量计算"></a>信息量计算</h3><p>$$ H=-(P_1logP_1+P_2logP_2+···+P_nlogP_n) $$</p><p>\(H\) 的专业术语称之为<strong>信息熵</strong>，单位为<strong>比特</strong>。</p><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>$$ H(X)=\sum_{x\epsilon X} P(x)logP(x) $$</p><blockquote><p>信息和消除不确定性是相联系的，信息熵越大，不确定性就越大。</p></blockquote><!-- ## 划分依据 --><h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>特征 A 对训练数据集 D 的信息增益 g(D,A) 定义为集合 D 的信息熵 H(D) 与特征 A 给定条件下 D 的信息条件熵 H(D|A) 之差。</p><h3 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h3><p>$$ g(D,A)=H(D)-H(D|A) $$</p><blockquote><p>信息增益表示得知特征 X 的信息而使得类 Y 的信息的不确定性减少的程度。</p></blockquote><h2 id="sklearn-决策树-API"><a href="#sklearn-决策树-API" class="headerlink" title="sklearn 决策树 API"></a>sklearn 决策树 API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.tree.DecisionTreeClassifier</span><br></pre></td></tr></table></figure><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeClassifier(criterion=<span class="string">'gini'</span>, max_depth=<span class="literal">None</span>, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul><li>criterion 默认是’gini’系数，也可以选择信息增益的熵’entropy’</li><li>max_depth 树的深度大小</li><li>random_state 随机数种子</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul><li>decision_path 返回决策树的路径</li></ul><h2 id="决策树的结构可视化"><a href="#决策树的结构可视化" class="headerlink" title="决策树的结构可视化"></a>决策树的结构可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.tree.export_graphviz()</span><br><span class="line">export_graphviz(estimator, out_file=<span class="string">'tree.dot'</span>, feature_names=[<span class="string">''</span>,<span class="string">''</span>])</span><br></pre></td></tr></table></figure><blockquote><p>estimator 估计器对象<br>out_file 保存路径<br>feature_names 特征处理后的特征名</p></blockquote><p>安装 graphviz 工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install graphviz</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dot -Tpng tree.dot -o tree.png</span><br></pre></td></tr></table></figure><h2 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h2><h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ul><li>简单的理解和解释，树木可视化</li><li>需要很少的数据准备，其他算法通常需要数据归一化</li></ul><h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ul><li>决策树学习者可以创建不能很好地推广数据的过于复杂的树，被称之为<strong>过拟合</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; 
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>模型选择与调优</title>
    <link href="http://stark365.com/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98/"/>
    <id>http://stark365.com/模型选择与调优/</id>
    <published>2019-03-31T08:14:17.700Z</published>
    <updated>2019-04-01T02:08:58.029Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="模型选择与调优"><a href="#模型选择与调优" class="headerlink" title="模型选择与调优"></a>模型选择与调优</h1><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>为了让被评估的模型更加精确可信。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>把训练集数据分成 n 等份，每一份包含训练集和验证集，求每一份模型准确率再计算平均值。</p><h2 id="超参数搜索-网格搜索"><a href="#超参数搜索-网格搜索" class="headerlink" title="超参数搜索/网格搜索"></a>超参数搜索/网格搜索</h2><p>通常情况下，很多算法参数需要手动指定，如 k-近邻算法 中的 k 值，这种参数叫<strong>超参数</strong>。但是手动调制过程繁杂，所以需要对模型进行预设几种超参数组合，每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV</span><br></pre></td></tr></table></figure><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.GridSearchCV(estimator, param_grid=<span class="literal">None</span>, cv=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数：</p><ul><li>estimator: 估计器对象</li><li>param_grid：估计器参数字典：{“n_neighbors”: [1, 3, 5]}</li><li>cv：交叉验证折数</li></ul><p>方法：</p><ul><li>fit: 输入训练数据</li><li>score: 准确率</li><li>best_score_: 在交叉验证中最高准确率</li><li>best_estimator_: 最好的参数模型</li><li>cv_results_: 每次交叉验证后的验证集准确率结果和训练集准确率结果</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;模型选择与调优&quot;&gt;&lt;a href=&quot;#模
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>分类模型评估</title>
    <link href="http://stark365.com/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/"/>
    <id>http://stark365.com/混淆矩阵/</id>
    <published>2019-03-31T07:41:57.604Z</published>
    <updated>2019-03-31T08:04:51.342Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="分类模型评估"><a href="#分类模型评估" class="headerlink" title="分类模型评估"></a>分类模型评估</h1><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)。</p><table><thead><tr><th></th><th style="text-align:right">预测正例</th><th style="text-align:center">预测假例</th></tr></thead><tbody><tr><td>真实正例</td><td style="text-align:right">真正例TP</td><td style="text-align:center">伪反例FN</td></tr><tr><td>真实假例</td><td style="text-align:right">伪正例FP</td><td style="text-align:center">真反例TN</td></tr></tbody></table><h2 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h2><p><strong>真实为正例</strong>的样本中预测结果为正例的比例。</p><table><thead><tr><th></th><th style="text-align:right">预测正例</th><th style="text-align:center">预测假例</th></tr></thead><tbody><tr><td>真实正例</td><td style="text-align:right"><strong><em>真正例TP</em></strong></td><td style="text-align:center"><strong><em>伪反例FN</em></strong></td></tr><tr><td>真实假例</td><td style="text-align:right">伪正例FP</td><td style="text-align:center">真反例TN</td></tr></tbody></table><h2 id="F1-score"><a href="#F1-score" class="headerlink" title="F1-score"></a>F1-score</h2><p>F1-score 反映了模型的稳健性。</p><p>$$ F1=\frac{2TP}{2TP+FN+FP}=\frac{2·Precision·Recall}{Precision+Recall} $$</p><h2 id="分类模型评估-API"><a href="#分类模型评估-API" class="headerlink" title="分类模型评估 API"></a>分类模型评估 API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report</span><br></pre></td></tr></table></figure><h2 id="分类模型评估用法"><a href="#分类模型评估用法" class="headerlink" title="分类模型评估用法"></a>分类模型评估用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.classification_report(y_true, y_pred, target_names=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><ul><li>y_true: 真实目标值</li><li>y_pred: 估计器预测目标值</li><li>target_names: 目标类别名称</li><li><strong><em>return</em></strong>: 每个类别精确率与召回率</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;分类模型评估&quot;&gt;&lt;a href=&quot;#分类
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="http://stark365.com/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://stark365.com/朴素贝叶斯/</id>
    <published>2019-03-31T01:53:10.007Z</published>
    <updated>2019-03-31T06:25:09.661Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><p>朴素贝叶斯算法基于条件概率，特征与特征之间相互独立互不影响。</p><h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2><p>$$ P(C|W)=\frac{P(W|C)P(C)}{P(W)} $$</p><blockquote><p>\(W\)为给定文档的特征值（频数统计，预测文档提供），\(C\)为文档类别。</p></blockquote><p>公式可理解为：</p><p>$$ P(C|F_1,F_2,…)=\frac{P(F_1,F_2,…|C)P(C)}{P(F_1,F_2,…)} $$</p><blockquote><p>其中\(C\)可以是不同类别。</p></blockquote><h3 id="公式举例"><a href="#公式举例" class="headerlink" title="公式举例"></a>公式举例</h3><ul><li><p>\(P(C)\)为每个文档类别的概率（某文档类别数/总文档数）</p></li><li><p>\(P(W|C)\)为给定类别下特征（被预测文档中出现的词）的概率</p><ul><li>计算方法：\(P(F_1|C)=\frac{N_I}{N}\)</li><li>\(N_i\)为该\(F_1\)词在\(C\)类别所有文档中出现的次数</li><li>\(N\)为所属类别\(C\)下的文档所有词出现的次数和</li></ul></li><li><p>\(P(F_1,F_2,…)\)预测文档中每个词的概率</p></li></ul><p>在贝叶斯公式中会因为某个特征为0而导致最终概率结果为0，为避免这种不合理情况的发生，<strong>需要添加拉普拉斯平滑系数</strong>。</p><h2 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h2><p>公式：</p><p>$$ P(F_1|C)=\frac{N_I+\alpha}{N+\alpha m} $$</p><blockquote><p>\(\alpha\)为指定的系数，一般为1，m为训练文档中统计出的 <strong>特征词</strong> 个数。</p></blockquote><h2 id="sklearn-API"><a href="#sklearn-API" class="headerlink" title="sklearn API"></a>sklearn API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.vaive_bayes.MultinomialNB</span><br></pre></td></tr></table></figure><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MultinomialNB(alpha=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><blockquote><p>alpha 为拉普拉斯平滑系数</p></blockquote><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>若训练集误差较大，则会严重影响结果。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul><li>优点：<ul><li>具有稳定的分类效率</li><li>对缺失数据不太敏感</li><li>分类准确度高</li></ul></li><li>缺点：<ul><li>当样本属性具有关联性时效果会下降</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;朴素贝叶斯算法&quot;&gt;&lt;a href=&quot;#朴
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>概率基础</title>
    <link href="http://stark365.com/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/"/>
    <id>http://stark365.com/概率基础/</id>
    <published>2019-03-31T01:11:26.005Z</published>
    <updated>2019-03-31T05:48:35.488Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="概率基础"><a href="#概率基础" class="headerlink" title="概率基础"></a>概率基础</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>概率定义为一件事情发生的可能性。</p><h1 id="联合概率和条件概率"><a href="#联合概率和条件概率" class="headerlink" title="联合概率和条件概率"></a>联合概率和条件概率</h1><h2 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h2><blockquote><p>包含多个条件，且所有条件同时成立的概率</p></blockquote><p>记做：\(P(A,B)=P(A)P(B)\)</p><h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><blockquote><p>事件 A 在另外一个事件 B 已经发生条件下的发生概率</p></blockquote><p>记做：\(P(A|B)\)</p><p>特性：\(P(A_1,A_2|B)=p(A_1|B)P(A_2|B)\)</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p><strong>此条件概率的成立，是由于 \(A_1,A_2\)相互独立的结果</strong>，相互独立即双方条件不相互影响。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;概率基础&quot;&gt;&lt;a href=&quot;#概率基础
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>K-近邻算法/KNN 算法</title>
    <link href="http://stark365.com/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://stark365.com/K-近邻算法/</id>
    <published>2019-03-29T01:02:57.792Z</published>
    <updated>2019-03-31T01:58:02.340Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="K-近邻算法-KNN-算法"><a href="#K-近邻算法-KNN-算法" class="headerlink" title="K-近邻算法/KNN 算法"></a>K-近邻算法/KNN 算法</h1><p>如果一个样本在特征空间中的 k 个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</p><p>最近邻居法采用<strong>向量空间模型</strong>来分类，概念为相同类别的案例，彼此的相似度高，而可以借由计算与已知类别案例之相似度，来评估未知类别案例可能的分类。</p><h2 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h2><h3 id="公式：-sqrt-a-1-b-1-2-a-2-b-2-2-a-3-b-3-2"><a href="#公式：-sqrt-a-1-b-1-2-a-2-b-2-2-a-3-b-3-2" class="headerlink" title="公式：  $$\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + (a_3 - b_3)^2}$$"></a>公式：  $$\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + (a_3 - b_3)^2}$$</h3><h2 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h2><p><strong>K-近邻算法需要做标准化处理</strong></p><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier</span><br></pre></td></tr></table></figure><h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KNeighborsClassifier(n_neighbors=<span class="number">5</span>, algorithm=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>n_neighbors: int</strong> 查询默认使用的邻居数。</p></blockquote><blockquote><p><strong>algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}</strong> 可选用于计算最近邻居的算法，’ball_tree’ 将使用 BallTree，’kd_tree’ 将使用 KDTree。’auto’ 将尝试根据传递给 fit 方法的值来决定最合适的算法。</p></blockquote><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>计算量大，性能问题严峻。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;K-近邻算法-KNN-算法&quot;&gt;&lt;a hr
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>算法分类</title>
    <link href="http://stark365.com/%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB/"/>
    <id>http://stark365.com/算法分类/</id>
    <published>2019-03-28T08:04:21.122Z</published>
    <updated>2019-04-02T05:15:39.440Z</updated>
    
    <content type="html"><![CDATA[<h1 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h1><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>有特征值和目标值。</p><p>算法：</p><ul><li>分类：目标值为离散型<ul><li><a href="../K-近邻算法">K-近邻算法</a></li><li><a href="../朴素贝叶斯">朴素贝叶斯算法</a></li><li><a href="../决策树">决策树</a></li><li><a href="../随机森林">随机森林</a></li><li><a href>逻辑回归</a></li><li><a href>神经网络</a></li></ul></li><li>回归：目标值为连续型，目标值过多<ul><li><a href="../线性回归">线性回归</a></li><li><a href>岭回归</a></li></ul></li></ul><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>只有特征值。</p><p>算法：</p><ul><li><a href>聚类</a></li><li><a href>k-means</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;算法分类&quot;&gt;&lt;a href=&quot;#算法分类&quot; class=&quot;headerlink&quot; title=&quot;算法分类&quot;&gt;&lt;/a&gt;算法分类&lt;/h1&gt;&lt;h2 id=&quot;监督学习&quot;&gt;&lt;a href=&quot;#监督学习&quot; class=&quot;headerlink&quot; title=&quot;监督学习&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>数据类型</title>
    <link href="http://stark365.com/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>http://stark365.com/数据类型/</id>
    <published>2019-03-28T07:54:52.096Z</published>
    <updated>2019-03-30T09:44:44.223Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h1><ul><li>离散型数据</li><li>连续型数据</li></ul><h2 id="离散型数据"><a href="#离散型数据" class="headerlink" title="离散型数据"></a>离散型数据</h2><p>由记录不同类别个体的数目所得到的数据，计数数据，所有数据全部为整数，而且不能再细分，也不能再进一步的提升精确度。</p><h2 id="连续型数据"><a href="#连续型数据" class="headerlink" title="连续型数据"></a>连续型数据</h2><p>变量可在某个范围内取任意数，可连续，如长度、时间、质量等，这类数通常是非整数，含有小数部分。</p><hr><p><strong>离散型数据区间内不可再分，而连续型数据区间内可再分</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据类型&quot;&gt;&lt;a href=&quot;#数据类型&quot; class=&quot;headerlink&quot; title=&quot;数据类型&quot;&gt;&lt;/a&gt;数据类型&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;离散型数据&lt;/li&gt;
&lt;li&gt;连续型数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;离散型数据&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Scikit-Learn 数据集 API</title>
    <link href="http://stark365.com/sklearn%E6%95%B0%E6%8D%AE%E9%9B%86API/"/>
    <id>http://stark365.com/sklearn数据集API/</id>
    <published>2019-03-28T05:45:22.517Z</published>
    <updated>2019-03-30T09:46:26.195Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Scikit-Learn-数据集-API"><a href="#Scikit-Learn-数据集-API" class="headerlink" title="Scikit-Learn 数据集 API"></a>Scikit-Learn 数据集 API</h1><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.datasets</span><br></pre></td></tr></table></figure><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datasets.load_*()  <span class="comment"># 获取 sklearn 内置的小型数据集。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datasets_fetch_*(data_home=<span class="literal">None</span>)  <span class="comment"># 获取大规模数据集，从互联网上下载，data_home 需要指定下载目录。</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Scikit-Learn-数据集-API&quot;&gt;&lt;a href=&quot;#Scikit-Learn-数据集-API&quot; class=&quot;headerlink&quot; title=&quot;Scikit-Learn 数据集 API&quot;&gt;&lt;/a&gt;Scikit-Learn 数据集 API&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>主成分分析 PCA</title>
    <link href="http://stark365.com/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    <id>http://stark365.com/主成分分析/</id>
    <published>2019-03-28T01:49:02.490Z</published>
    <updated>2019-03-30T09:45:54.995Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析 PCA"></a>主成分分析 PCA</h1><p>PCA 是一种分析、简化数据集的技术,目的是数据降维压缩，尽可能降低原数据的维数复杂度，会<strong>损失少量信息</strong>。</p><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.decomposition</span><br></pre></td></tr></table></figure><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PCA(n_components=<span class="number">0.9</span>) <span class="comment"># 小数表示特征信息保留百分比，整数表示特征保留数</span></span><br><span class="line">PCA.fit_transform(X)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;主成分分析-PCA&quot;&gt;&lt;a href=&quot;#主成分分析-PCA&quot; class=&quot;headerlink&quot; title=&quot;主成分分析 PCA&quot;&gt;&lt;/a&gt;主成分分析 PCA&lt;/h1&gt;&lt;p&gt;PCA 是一种分析、简化数据集的技术,目的是数据降维压缩，尽可能降低原数据的维数复杂
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="http://stark365.com/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>http://stark365.com/特征选择/</id>
    <published>2019-03-28T01:18:15.752Z</published>
    <updated>2019-03-30T09:45:33.185Z</updated>
    
    <content type="html"><![CDATA[<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>部分特征的相关度高，容易消耗计算机性能；部分特征对预测结果有影响。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><ul><li>Filter(过滤式):VarianceThreshold</li><li>Embedded(嵌入式):正则化、决策树</li><li>Wrapper(包裹式)</li></ul><h2 id="Filter-过滤式"><a href="#Filter-过滤式" class="headerlink" title="Filter(过滤式)"></a>Filter(过滤式)</h2><p>从方差大小考虑特征数据情况</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.feature_selection.VarianceThreshold</span><br></pre></td></tr></table></figure><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">VarianceThreshold(threshold = <span class="number">0.0</span>)</span><br><span class="line">Variance.fit_transform(<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;特征选择&quot;&gt;&lt;a href=&quot;#特征选择&quot; class=&quot;headerlink&quot; title=&quot;特征选择&quot;&gt;&lt;/a&gt;特征选择&lt;/h1&gt;&lt;p&gt;部分特征的相关度高，容易消耗计算机性能；部分特征对预测结果有影响。&lt;/p&gt;
&lt;h2 id=&quot;主要方法&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>标准缩放</title>
    <link href="http://stark365.com/%E5%BD%92%E4%B8%80%E5%8C%96%E6%A0%87%E5%87%86%E5%8C%96/"/>
    <id>http://stark365.com/归一化标准化/</id>
    <published>2019-03-27T06:48:29.554Z</published>
    <updated>2019-03-30T09:44:31.004Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h1 id="标准缩放"><a href="#标准缩放" class="headerlink" title="标准缩放"></a>标准缩放</h1><h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a><strong>归一化</strong></h2><p>当多个特征同等重要的时候，需要进行归一化。使得某一个特征对最终结果不会造成较大影响。比如某个特征比其他特征位数大得多，导致其他特征对结果几乎没有影响时，则需要使用归一化。</p><h3 id="公式：-X’-frac-x-min-max-min-X’’-X’-mx-mi-mi"><a href="#公式：-X’-frac-x-min-max-min-X’’-X’-mx-mi-mi" class="headerlink" title="公式： $$X’=\frac{x-min}{max-min}$$ $$X’’=X’*(mx-mi)+mi$$"></a>公式： $$X’=\frac{x-min}{max-min}$$ $$X’’=X’*(mx-mi)+mi$$</h3><blockquote><p>x 为特征值，min 为最小特征值， max 为最大特征值，X’’ 为归一化后结果值，mx 为结果区间最大值， mi 为结果区间最小值</p></blockquote><p>归一化公式中最大值与最小值容易受到异常点影响，所以这种方法<strong>鲁棒性</strong>较差，只适合传统精确小数据场景。</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.preprocessing.MinMaxScaler</span><br></pre></td></tr></table></figure><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MinMaxScalar(feature_range=(<span class="number">0</span>,<span class="number">1</span>))  <span class="comment"># 缩小范围</span></span><br><span class="line">MinMaxScalar.fit_transform(X)</span><br></pre></td></tr></table></figure><h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a><strong>标准化</strong></h2><p>通过对原始数据进行变换把数据变换到均值为0，方差为1范围内。</p><h3 id="公式：-var-frac-x1-mean-2-x2-mean-2-···-n-每个特征的样本数-sigma-sqrt-var-X’-frac-x-mean-sigma"><a href="#公式：-var-frac-x1-mean-2-x2-mean-2-···-n-每个特征的样本数-sigma-sqrt-var-X’-frac-x-mean-sigma" class="headerlink" title="公式：   $$var=\frac{(x1-mean)^2+(x2-mean)^2+···}{n(每个特征的样本数)}$$ $$\sigma=\sqrt{var}$$  $$X’=\frac{x-mean}{\sigma}$$"></a>公式：   $$var=\frac{(x1-mean)^2+(x2-mean)^2+···}{n(每个特征的样本数)}$$ $$\sigma=\sqrt{var}$$  $$X’=\frac{x-mean}{\sigma}$$</h3><blockquote><p>mean 为平均值，σ 为标准差，var 为方差</p></blockquote><p>标准化中标准差可以应对异常点的数值突出，影响较小。在已有<strong>样本足够多的情况下比较稳定</strong>，适合现代嘈杂大数据场景。</p><h3 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.preprocessing.StandardScaler</span><br></pre></td></tr></table></figure><h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sta = StandardScaler()</span><br><span class="line">sta.fit_stansform(X)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;标准缩放&quot;&gt;&lt;a href=&quot;#标准缩放
      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习笔记</title>
    <link href="http://stark365.com/README/"/>
    <id>http://stark365.com/README/</id>
    <published>2018-10-22T16:11:18.000Z</published>
    <updated>2019-04-02T06:57:32.929Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习笔记"><a href="#机器学习笔记" class="headerlink" title="机器学习笔记"></a>机器学习笔记</h1><ul><li>数据特征预处理<ul><li><a href="/归一化标准化">标准缩放</a></li></ul></li><li>数据降维<ul><li><a href="./特征选择">特征选择</a></li><li><a href="./主成分分析">主成分分析</a></li></ul></li><li>算法简述<ul><li><a href="./数据类型">数据类型</a></li><li><a href="./算法分类">算法分类</a></li></ul></li><li>监督学习<ul><li>分类算法<ul><li><a href="./sklearn数据集API">scikit-learn 数据集 API </a></li><li><a href="./K-近邻算法">K-近邻/KNN</a></li><li><a href="./朴素贝叶斯">朴素贝叶斯</a><ul><li><a href="./概率基础">概率基础</a></li><li><a href="./混淆矩阵">分类模型评估</a></li><li><a href="./模型选择与调优">模型选择与调优</a></li></ul></li><li><a href="./决策树">决策树</a></li><li><a href="./随机森林">随机森林</a></li></ul></li><li>回归算法<ul><li><a href="./线性回归">线性回归</a></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器学习笔记&quot;&gt;&lt;a href=&quot;#机器学习笔记&quot; class=&quot;headerlink&quot; title=&quot;机器学习笔记&quot;&gt;&lt;/a&gt;机器学习笔记&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;数据特征预处理&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/归一化标准化&quot;&gt;标准缩放&lt;/a&gt;&lt;/li&gt;

      
    
    </summary>
    
      <category term="机器学习笔记" scheme="http://stark365.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
